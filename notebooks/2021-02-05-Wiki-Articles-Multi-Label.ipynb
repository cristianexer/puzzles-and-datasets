{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Wikipedia Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dataset](https://www.kaggle.com/urbanbricks/wikipedia-promotional-articles)\n",
    "\n",
    "**Context**\n",
    "\n",
    "The growing availability of information in the past decade has allowed internet users to find vast amounts of information online, but this has come with more and more deceptive articles designed to advertise or promote a product or ideology. In addition, hidden sponsored news articles have grown in prevalence in recent years as news organizations have shifted their business strategy to account for developments in technology and content consumption. It is for this reason that having a system in place to detect these deceptive practices is more important than ever.\n",
    "\n",
    "**Content**\n",
    "\n",
    "This dataset consists of articles that were tagged by users as having a \"promotional tone\" (promotional.csv) and of articles that were tagged as \"good articles\" (good.csv).\n",
    "\n",
    "**The each promotional article can have multiple labels (quotes from Wikipedia tags):**\n",
    "\n",
    "- **advert** - \"This article contains content that is written like an advertisement.\"\n",
    "- **coi** - \"A major contributor to this article appears to have a close connection with its subject.\"\n",
    "- **fanpov** - \"This article may be written from a fan's point of view, rather than a neutral point of view.\"\n",
    "- **pr** - \"This article reads like a press release or a news article or is largely based on routine coverage or sensationalism.\"\n",
    "- **resume** - \"This biographical article is written like a rÃ©sumÃ©.\"\n",
    "\n",
    "The \"good articles\" are articles that were deemed \"well written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>advert</th>\n",
       "      <th>coi</th>\n",
       "      <th>fanpov</th>\n",
       "      <th>pr</th>\n",
       "      <th>resume</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1 Litre no Namida 1, lit. 1 Litre of Tears als...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1%20Litre%20no%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1DayLater was free, web based software that wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1DayLater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1E is a privately owned IT software and servic...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1Malaysia pronounced One Malaysia in English a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Jerusalem Biennale, as stated on the Bienn...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1st%20Jerusalem%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23832</th>\n",
       "      <td>23832</td>\n",
       "      <td>ZURICH.MINDS is a non profit foundation set up...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zurich.minds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23833</th>\n",
       "      <td>23833</td>\n",
       "      <td>zvelo, Inc. or simply zvelo is a privately hel...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zvelo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23834</th>\n",
       "      <td>23834</td>\n",
       "      <td>Zygote Media Group is a 3D human anatomy conte...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zygote%20Media%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23835</th>\n",
       "      <td>23835</td>\n",
       "      <td>Zylom is a distributor of casual games for PC ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zylom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23836</th>\n",
       "      <td>23836</td>\n",
       "      <td>Zynx Health Incorporated is an American corpor...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Zynx%20Health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23837 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  advert  coi  \\\n",
       "0          0  1 Litre no Namida 1, lit. 1 Litre of Tears als...       0    0   \n",
       "1          1  1DayLater was free, web based software that wa...       1    1   \n",
       "2          2  1E is a privately owned IT software and servic...       1    0   \n",
       "3          3  1Malaysia pronounced One Malaysia in English a...       1    0   \n",
       "4          4  The Jerusalem Biennale, as stated on the Bienn...       1    0   \n",
       "...      ...                                                ...     ...  ...   \n",
       "23832  23832  ZURICH.MINDS is a non profit foundation set up...       1    0   \n",
       "23833  23833  zvelo, Inc. or simply zvelo is a privately hel...       1    0   \n",
       "23834  23834  Zygote Media Group is a 3D human anatomy conte...       1    1   \n",
       "23835  23835  Zylom is a distributor of casual games for PC ...       1    0   \n",
       "23836  23836  Zynx Health Incorporated is an American corpor...       1    1   \n",
       "\n",
       "       fanpov  pr  resume                                                url  \n",
       "0           1   0       0  https://en.wikipedia.org/wiki/1%20Litre%20no%2...  \n",
       "1           0   0       0            https://en.wikipedia.org/wiki/1DayLater  \n",
       "2           0   0       0                   https://en.wikipedia.org/wiki/1E  \n",
       "3           0   0       0            https://en.wikipedia.org/wiki/1Malaysia  \n",
       "4           0   0       0  https://en.wikipedia.org/wiki/1st%20Jerusalem%...  \n",
       "...       ...  ..     ...                                                ...  \n",
       "23832       0   0       0         https://en.wikipedia.org/wiki/Zurich.minds  \n",
       "23833       0   0       0                https://en.wikipedia.org/wiki/Zvelo  \n",
       "23834       0   0       0  https://en.wikipedia.org/wiki/Zygote%20Media%2...  \n",
       "23835       0   0       0                https://en.wikipedia.org/wiki/Zylom  \n",
       "23836       0   0       0        https://en.wikipedia.org/wiki/Zynx%20Health  \n",
       "\n",
       "[23837 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/wiki-articles-promo.csv').reset_index().rename(columns={'index':'id'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['advert','coi','fanpov','pr','resume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df[['text'] + labels],test_size=0.33, random_state=42,stratify=df[labels[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For ML based models we can use USE to extract the sentence vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n",
    "\n",
    "The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder.\n",
    "\n",
    "To learn more about text embeddings, refer to the TensorFlow Embeddings documentation. Our encoder differs from word level embedding models in that we train on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just individual words. Details are available in the paper \"Universal Sentence Encoder\"\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1803.11175)\n",
    "\n",
    "[Pre-Trained Models](https://tfhub.dev/google/universal-sentence-encoder/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow tensorflow-hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "class UniversalSentenceEncoder:\n",
    "\n",
    "    def __init__(self, encoder='universal-sentence-encoder', version='4'):\n",
    "        self.version = version\n",
    "        self.encoder = encoder\n",
    "        self.embd = hub.load(f\"https://tfhub.dev/google/{encoder}/{version}\",)\n",
    "\n",
    "    def embed(self, sentences):\n",
    "        return self.embd(sentences)\n",
    "\n",
    "    def squized(self, sentences):\n",
    "        return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string))))\n",
    "    \n",
    "use = UniversalSentenceEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the USE to get the vectors from the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 7min 41s, total: 13min 20s\n",
      "Wall time: 16min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_use = train.copy()\n",
    "train_use['text_vect'] = use.squized(train_use['text'].tolist()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 3min 4s, total: 6min 10s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_use = test.copy()\n",
    "test_use['text_vect'] = use.squized(test_use['text'].tolist()).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms a multi-label classification problem with L labels into L single-label separate binary classification problems using the same base classifier provided in the constructor. The prediction output is the union of all per label classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 1.21 s, total: 2min 44s\n",
      "Wall time: 2min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=RandomForestClassifier(), require_dense=[True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier = BinaryRelevance(\n",
    "    classifier = RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# train\n",
    "classifier.fit(pd.DataFrame(train_use['text_vect'].tolist()), train_use[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = classifier.predict(pd.DataFrame(test_use['text_vect'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions_proba = classifier.predict_proba(pd.DataFrame(test_use['text_vect'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90      6239\n",
      "           1       0.00      0.00      0.00       707\n",
      "           2       0.92      0.14      0.24       493\n",
      "           3       0.00      0.00      0.00       500\n",
      "           4       0.74      0.16      0.26       726\n",
      "\n",
      "   micro avg       0.83      0.72      0.77      8665\n",
      "   macro avg       0.50      0.25      0.28      8665\n",
      "weighted avg       0.72      0.72      0.68      8665\n",
      " samples avg       0.79      0.75      0.77      8665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_use[labels],predictions,zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:22:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:24:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:25:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 26min 45s, sys: 5.84 s, total: 26min 51s\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=XGBClassifier(base_score=None, booster=None,\n",
       "                                         colsample_bylevel=None,\n",
       "                                         colsample_bynode=None,\n",
       "                                         colsample_bytree=None, gamma=None,\n",
       "                                         gpu_id=None, importance_type='gain',\n",
       "                                         interaction_constraints=None,\n",
       "                                         learning_rate=None,\n",
       "                                         max_delta_step=None, max_depth=None,\n",
       "                                         min_child_weight=None, missing=nan,\n",
       "                                         monotone_constraints=None,\n",
       "                                         n_estimators=100, n_jobs=None,\n",
       "                                         num_parallel_tree=None,\n",
       "                                         random_state=None, reg_alpha=None,\n",
       "                                         reg_lambda=None, scale_pos_weight=None,\n",
       "                                         subsample=None, tree_method=None,\n",
       "                                         use_label_encoder=False,\n",
       "                                         validate_parameters=None,\n",
       "                                         verbosity=None),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier_xgb = BinaryRelevance(\n",
    "    classifier = xgb.XGBClassifier(objective='binary:logistic',use_label_encoder=False)\n",
    ")\n",
    "\n",
    "# train\n",
    "classifier_xgb.fit(pd.DataFrame(train_use['text_vect'].tolist()), train_use[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "xgb_pred = classifier_xgb.predict(pd.DataFrame(test_use['text_vect'].tolist(),columns=[f'f{x}' for x in range(512)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "xgb_pred_proba = classifier_xgb.predict_proba(pd.DataFrame(test_use['text_vect'].tolist(),columns=[f'f{x}' for x in range(512)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90      6239\n",
      "           1       0.45      0.01      0.01       707\n",
      "           2       0.73      0.29      0.42       493\n",
      "           3       0.00      0.00      0.00       500\n",
      "           4       0.61      0.38      0.47       726\n",
      "\n",
      "   micro avg       0.84      0.73      0.78      8665\n",
      "   macro avg       0.53      0.32      0.36      8665\n",
      "weighted avg       0.75      0.73      0.71      8665\n",
      " samples avg       0.79      0.76      0.77      8665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_use[labels],xgb_pred,zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see using `Binary Relevance` we can easily use diffrent models for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR4ooVfeAItrZhdZgvmT7OBSFkhNC-PvO-SYA&usqp=CAU)\n",
    "\n",
    "Not those..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original package](https://github.com/huggingface/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets then share them with the community on our model hub. At the same time, each python module defining an architecture can be used as a standalone and modified to enable quick research experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the two most popular deep learning libraries, PyTorch and TensorFlow, with a seamless integration between them, allowing you to train your models with one then load it for inference with the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/06/Screenshot-from-2019-06-17-20-01-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we will use the [Simple Transformers](https://github.com/ThilinaRajapakse/simpletransformers) package, which lets you quickly train and evaluate Transformer models. Keep the big guns for other projects and new PC..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1e14L2kLwrAl"
   },
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r8veMyOFwrAl"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3nw3-lrrwrAm"
   },
   "outputs": [],
   "source": [
    "train['labels'] = train[labels].values.tolist()\n",
    "test['labels'] = test[labels].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "03fRF2i4wrAm"
   },
   "outputs": [],
   "source": [
    "train_df = train[['text','labels']].copy()\n",
    "eval_df = test[['text','labels']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386,
     "referenced_widgets": [
      "d85bb2f0abb44ec19cea590b280da70f",
      "1e88412d546e4374a37dbeafe026c83f",
      "9db5f26d12d4479cb45a82b4ea6f3538",
      "0cd9a7b302b9422d88f1b478f7c932fc",
      "b7e102c579d342c5a314fa3f0546ced6",
      "74e6c3e5c6c841fbba245ec3087ab8d1",
      "6536fca8b41d48f392d0446a41a9ba6c",
      "c3ee5821ddc946829d95db63f444d9fa",
      "f6d06ada28494d21bde8803bd9f049ac",
      "ac4bba812f70430f97d09872d2cbd7be",
      "66b77375f4e74eb8b69c56ff4b649801",
      "f805aa62146b4aa184a38c338cbccdbd",
      "379bac51a08347f7898bd9cd69274ff9",
      "c1baa5ccdc6a4d63905b82ddac630cf1",
      "3ed958c391dd4ece9e2a93a0d95fda94",
      "dce5c372e1fe42fb8244acf2d4d1a376",
      "6258a72116294d7e95accd37d5aca05f",
      "8a3dc8ffae6b493ebf43f9563f4ad96b",
      "ef921aa881d945bfadeec59cc2dd5709",
      "d20f82a39d1146edbc03d2b5ff58e53d",
      "d3b82dfb72c84fa3857932cd1d182e2a",
      "edf3494be3324ae3802af04e5648faa7",
      "35141d3df16f44e78a50f02588ea9514",
      "da80af0141cc429eba9862d8e92ea0b1",
      "0ba9d038b13548ad9291e07cfc7a8f51",
      "6d90424c381a4247b50a8b17fc12b0bd",
      "0151315139d14d409e8f412730da3f40",
      "f7b52bd1a32b46ee83344f5dcb0801ae",
      "f71857fe95ee4d5588899f4c0c9f925c",
      "5d1f6e011bcb46de8b7eede9db3949ca",
      "bef84c4a163a44dc887694200e133093",
      "5753379d079546e6badd3b601b7ed1c9",
      "9ae327baca2c40a08b50d7b3c21e67be",
      "fed3140d1a6940bc81f8a9e09411721b",
      "0e80ae5067a041a0b847769dccca4381",
      "8f4f30c7e5894fa5974da5e0fd4670f5",
      "bb6277f49ebb452fb78ed85395c76c95",
      "c94a782b0eda4c808e514fc5b5b7db25",
      "c8906476d82c4e84a7895186a5fbd116",
      "4b06e5edcd4946998b4822a3fce9e88f",
      "5d42135b77e44b1c862ae5eb1b782cb7",
      "4253074745414e469abeaa0d4c9a243b",
      "d097be5a38ed460997f46bacf5f50349",
      "bc33282cfc17437680017bff2535efa7"
     ]
    },
    "id": "8-yBIGFvwrAm",
    "outputId": "93a39143-f10e-462e-e137-c76d2e4f8756"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140598834372336 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85bb2f0abb44ec19cea590b280da70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140598834372336 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n",
      "INFO:filelock:Lock 140597941182416 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f805aa62146b4aa184a38c338cbccdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140597941182416 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultiLabelSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:filelock:Lock 140597903954216 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35141d3df16f44e78a50f02588ea9514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140597903954216 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n",
      "INFO:filelock:Lock 140597938558176 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed3140d1a6940bc81f8a9e09411721b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140597938558176 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
     ]
    }
   ],
   "source": [
    "# Create a MultiLabelClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    \"roberta\",\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(labels),\n",
    "    use_cuda=True,\n",
    "    args={\"reprocess_input_data\": True, \"overwrite_output_dir\": True, \"num_train_epochs\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343,
     "referenced_widgets": [
      "8cfac930301541299ac05b436cac2097",
      "0794428983304260b286d8f3b05c4279",
      "bae209a4f27441c1bd241085719d5553",
      "b92bb118f0bf4b5e97006645ac99244a",
      "ca9ee589174e4227b9c71b1c56b6018a",
      "dda606ab54c944ab93aa3729a46e422d",
      "8724fcba367242efa1669d6b298f4cf4",
      "6ef9dd2e683249079346909625eeec12",
      "c032daf5f11e4d728dbca8e41e6f0c4a",
      "0e26772e78c24090872210fa3f588875",
      "8e4728d39dcf4b3795fc5bb0954f403f",
      "bfc5ceeea48f4780a7c5872fa6357d30",
      "7b3da6bc05434c029fdd0d1338b35c1a",
      "ded6e5c63fc943fe9cd9dac5ecedea6b",
      "8975af3dc58a4db8ab3b0cdde456cdfe",
      "d62678cbe25a461a96272974914350f1",
      "954d6f9315c44bc9b77f04ad2184ed6d",
      "53b6f71e0a09437d989c5fd54bf42cc3",
      "f99e3e4a6c214fdc81e8244ac489fb02",
      "a69adcdb88454a26a8b39a944ecdc2b1",
      "c583dc575695499ea8da91b384448fc8",
      "00daccd5cb4748dbb52bba6cf954baeb",
      "4b9d4bccf996443b8d53f62242295ae6",
      "8fbb10c294ef469ab1b9dd96a24898df",
      "1b7ae71fe3054811a5800031a67e5729",
      "437cc6231ca04cc4a774bc3024a2d519",
      "911622c0f4974955b60ffab048c237ea",
      "8f6bf8c991f245d98515e030eb0a94c7",
      "99606aec0e954c158788d6be3554fb0e",
      "3880020c99cd419298cb513f3cc2354a",
      "afa669ab86934020afc1734598a9c6ac",
      "06439027fc6a4391ba6866586a079ec7",
      "1062d4c5f35145dc89f73dc808285017",
      "1415986dc2644f68ad8a72f73fec4d71",
      "0f38acb5121a44a1b53c74fbb31b3969",
      "d69159c21ac04797aebd401dc73faa39",
      "0f6b1b8caa834a868485ed666871512d",
      "a06a4d1b7e0443d28f72ff53caac5e8c",
      "09ee0d45ec9042ec8e39c3d68d8b4476",
      "614d6ba1ea3947c3bda85de2842a04fb",
      "a7da34b57b4a4d0a8cae378c4dc5042d",
      "4140b11784ac4c19ac8fe3ad2c82c303",
      "3a3f6ea6e5584ed8aef6cb4ddb7bcd02",
      "a9f887790bc74839a9ae55419cb08f18",
      "9059b7b7988b45e1be94f9fe1e705d8c",
      "13b6cbb710ce4f5b8d068148dd71e6d4",
      "415adee353ca4d2d8ae62d3f5576c750",
      "a86a004b6e094aba9c521bc1596e7eb2",
      "4a3e13cc0d6a4d4c9b6830ae66f4f7f8",
      "01b895b53b4b4abf807c79306e6d199e",
      "799047c42e1b4aaeb179caa589ac35d0",
      "af53cf9e140140038b0f40b41f4e2709",
      "f577201e7413467891028a13133262fe",
      "30acde0606a940549f17ad68de2dabda",
      "f7e3c81dad6b40b8a8f466a9c90a3377",
      "8cb4eae88e2a40c38d02a1f607b1e7b6",
      "8fe534c9aba74391ab4b94e0d0b28731",
      "7a744c76a325422994765c1b2b311c90",
      "cd230aa103ca4f3fa74c50f8fe3750f2",
      "ba22dcc8a10a46468f3cc99af145c422",
      "f8da3db4756c44ac9b786dcd00801f5a",
      "aaf2941477e748a6b0a90e9dc12bfb70",
      "295afe64b0f94cc284a900163e378030",
      "8c54ef5687a6496db73c3592dd760d1a",
      "421424f11e514245a89b343c12dbe4a8",
      "6cc90cf826614f018c3a6a3cd71fb5bd",
      "e0f308a3c36b45f09ccf0addc2c8e53f",
      "75afecf594e54c1bb8eeffa26453eace",
      "96e4d0ae427e4ce19ed071237bd0a63e",
      "8c4fc78e21374232b32830d182265842",
      "25522dc630f24e1b93b4ffd3219adee1",
      "d970ead4931b45df81c189737a48237a",
      "39e8907978f94894bc0693fec6d14609",
      "6d5346d40ec147558a7d87d2b1d524a7",
      "c7e513184e504da680de728301c0b4c0",
      "e5846c2029fe4e468b2311b71c5c800a",
      "b1f0283f1401486aa540946a7bdcc635"
     ]
    },
    "id": "cMClsPQ4wrAn",
    "outputId": "3179f191-e2c9-40c8-d6bc-2fb23d487d5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfac930301541299ac05b436cac2097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_roberta_128_0_15970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc5ceeea48f4780a7c5872fa6357d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9d4bccf996443b8d53f62242295ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 5:   0%|          | 0/1997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1415986dc2644f68ad8a72f73fec4d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/1997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9059b7b7988b45e1be94f9fe1e705d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/1997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb4eae88e2a40c38d02a1f607b1e7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 5:   0%|          | 0/1997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f308a3c36b45f09ccf0addc2c8e53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 5:   0%|          | 0/1997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Training of roberta model complete. Saved to outputs/.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9985, 0.23426581945831798)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "47bb5a0d74a6468dbd68f8f2090c4e1f",
      "e08eefa6ddc64edf839ec167737ab856",
      "5a83e1d76c5e479191127618870ad422",
      "eae88abbb5b7439ab517140ce3ee2b7e",
      "cdfad6171f1e47e6b824ad5573cfe2e6",
      "de72d73618f646f8a2162be8a6ec2f1b",
      "ead31adce657423d9e774e45b8592a89",
      "effe0b76715b41f2938ab690503bd55b",
      "77e4add121f241baa09fec65e78214c3",
      "c3e7c6ad53054727adf99add3f11800f",
      "9dedf055c76d40fe95dcab5d2fc19c0c",
      "6d0b11a5c183436a895b6c5412b96717",
      "dcdd70b7df4b42f788396a8188cbdc06",
      "3cc44a89debf4647bf2e25aef71e9f9f",
      "f942d5e6da674178ae34b6102149a70e",
      "589164d95af246e091d8ba52a497959a",
      "33d4d6597c7443edb3ce59ee8b04eaf8",
      "7174ddd18f6c4d90a8e779706ea2401e",
      "b9f00f1148b04390ab765b94448018de",
      "56ea925f97ea4563bc32e5e49f218247",
      "aa919d3602d843978cbab29086ba3537",
      "afb4b8f6abbb4e51bbf09d656ee82424"
     ]
    },
    "id": "ezNaEXWpwrAn",
    "outputId": "a07196f2-a6f9-408c-e6c3-9c03dde70fa5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bb5a0d74a6468dbd68f8f2090c4e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_dev_roberta_128_0_7867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0b11a5c183436a895b6c5412b96717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KY8BGmv0T58y",
    "outputId": "8cc00d38-3604-4ad9-f7d9-0184e6ffa10a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LRAP': 0.8831122975015083, 'eval_loss': 0.28330832859690536}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "VxUDtJBQT8Iz",
    "outputId": "4ef724cf-3241-4caa-abca-e818b4d20f72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advert</th>\n",
       "      <th>coi</th>\n",
       "      <th>fanpov</th>\n",
       "      <th>pr</th>\n",
       "      <th>resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963867</td>\n",
       "      <td>0.109924</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.044006</td>\n",
       "      <td>0.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.969727</td>\n",
       "      <td>0.089905</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>0.001156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.037048</td>\n",
       "      <td>0.001165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.972168</td>\n",
       "      <td>0.069641</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.031860</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973145</td>\n",
       "      <td>0.062805</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.029816</td>\n",
       "      <td>0.001040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7862</th>\n",
       "      <td>0.938965</td>\n",
       "      <td>0.149658</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.074341</td>\n",
       "      <td>0.001810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7863</th>\n",
       "      <td>0.087708</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.053314</td>\n",
       "      <td>0.870605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>0.970215</td>\n",
       "      <td>0.038910</td>\n",
       "      <td>0.007904</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.001086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>0.969238</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>0.971191</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.026810</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7867 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        advert       coi    fanpov        pr    resume\n",
       "0     0.963867  0.109924  0.001567  0.044006  0.001279\n",
       "1     0.969727  0.089905  0.002100  0.036438  0.001156\n",
       "2     0.969238  0.091858  0.002035  0.037048  0.001165\n",
       "3     0.972168  0.069641  0.003004  0.031860  0.001057\n",
       "4     0.973145  0.062805  0.003622  0.029816  0.001040\n",
       "...        ...       ...       ...       ...       ...\n",
       "7862  0.938965  0.149658  0.001240  0.074341  0.001810\n",
       "7863  0.087708  0.089600  0.006958  0.053314  0.870605\n",
       "7864  0.970215  0.038910  0.007904  0.025513  0.001086\n",
       "7865  0.969238  0.035828  0.009193  0.025421  0.001078\n",
       "7866  0.971191  0.041138  0.007122  0.026810  0.001032\n",
       "\n",
       "[7867 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame(model_outputs,columns=labels)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcAAf1c1C-Co",
    "outputId": "9a1c5471-5e1f-4bc9-f019-9bb99ceb80c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90      6239\n",
      "           1       0.00      0.00      0.00       707\n",
      "           2       0.60      0.43      0.50       493\n",
      "           3       0.30      0.02      0.03       500\n",
      "           4       0.55      0.53      0.54       726\n",
      "\n",
      "   micro avg       0.83      0.74      0.78      8665\n",
      "   macro avg       0.46      0.38      0.39      8665\n",
      "weighted avg       0.72      0.74      0.72      8665\n",
      " samples avg       0.81      0.77      0.78      8665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test[labels],preds.gt(.5).astype(int),zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have played with those packages to solve the multi-label classificationt task\n",
    "- Binary Relevance\n",
    "    - Universal Sentence Encoder\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "- Transformers\n",
    "\n",
    "We could see very intresting results, especially that each one of them works very diffrent from each other.\n",
    "\n",
    "Personally I think using the XGBoost model with the USE worked the best in terms of accuracy-resources report, also it had a better precision for the label `1` where the other models could not score anything for that label.\n",
    "\n",
    "All these models can be improved by tunning but this notebooks was mainly designed for learning purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
